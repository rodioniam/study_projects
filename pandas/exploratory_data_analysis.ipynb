{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.txt', encoding='windows-1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae99806",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_columns = ['DR_Dat', 'DR_Tim', 'DR_NChk', 'DR_NDoc', \n",
    "                'DR_Apt', 'DR_NDrugs', 'DR_Kol', 'DR_CZak', 'DR_CRoz', \n",
    "                'DR_SDisc', 'DR_TPay', 'DR_CDrugs',  \n",
    "                'DR_Suppl','DR_CDisc', 'DR_BCDisc', 'DR_TabEmpl',\n",
    "                'DR_VZak', 'DR_Pos']\n",
    "\n",
    "df = df[user_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed11856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['date', 'time', 'n_chk', 'n_doc', 'apt', 'name_drugs',\n",
    "       'amnt', 'purch_price', 'retail_price', 'disc', 'pay_type', 'drug_id',\n",
    "       'supplier', 'disc_id', 'disc_barcode', 'employee', 'vzak', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72843a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disc_barcode'] = df['disc_barcode'].astype('str').replace(r'\\.0', '', regex=True)\n",
    "df['date']= pd.to_datetime(df['date']).dt.strftime('%d.%m.%Y')\n",
    "df['vzak'] = df['vzak'].astype('str').replace('1', 'offline').replace('2', 'online')\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bab372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function shows structure of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ded9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i can use describe method to get most of the descriptive parameters of the dataframe\n",
    "df.describe()\n",
    "# this method only shows non-objective type columns - only int and float type columns will be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to work with categorical data i need to use construction like this\n",
    "df['pay_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i also can call column by .\n",
    "df.pay_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9334a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apt.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3010f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % from all\n",
    "df.apt.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a histogram to estimate a distribution\n",
    "df.pos.value_counts() # this does almost what i need, but its not fancy\n",
    "df.pos.hist() # will build histogram\n",
    "df.pos.hist(bins=24) # i can set custom bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53810e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec73773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas couldnt convert it to float, stated that this is str\n",
    "df['time'] = pd.to_datetime(df['time']).dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb52274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas couldnt convert it to float, stated that this is str. I clearly changed type\n",
    "df['name_drugs'] = df.name_drugs.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas couldnt convert it to float, stated that this is str. I clearly changed type, but it didnt help\n",
    "df['supplier'] = df.supplier.astype('str').replace('\\\"', '', regex=True).replace(' ', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what it sees as number columns\n",
    "df.select_dtypes(include='number').columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of ways to find correlation is to build heatmap\n",
    "# i need to give correlation matrix of my dataframe\n",
    "plt.figure(figsize=(16, 6)) # this way i can customize the look of heatmap\n",
    "sns.heatmap(df.corr(numeric_only=True), vmax=1, vmin=-1, annot=True) # specified that it should use only numeric columns\n",
    "# also added vmax and vmin to clearly see positive and negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i created new column so i can evaluate the distribution of sold items by hour\n",
    "df['hour'] = pd.to_datetime(df.time).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in my data file i need to count column 'amnt' by each hour and see distribution\n",
    "ds = df.groupby(['hour'])['amnt'].agg('sum')\n",
    "ds = df.groupby(['hour']).agg({'amnt': 'sum'}) # does the same\n",
    "# logic is similar to sql -- im summing column 'amnt' and grouping it by column 'hour'\n",
    "# im assigning it to a new variable since i need to male boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff908870",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 6))\n",
    "sns.boxplot(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda102c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is more complex example of grouping\n",
    "# i can set rules of agg in dictionary - column name would be a key and value is what i need to do (count, sum, max, etc)\n",
    "a = df.groupby(['date', 'n_chk'])[['amnt', 'retail_price', 'purch_price']].agg({\n",
    "    'amnt': 'sum', \n",
    "    'retail_price': ['sum', 'max'],\n",
    "    'purch_price': 'sum'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()\n",
    "# here the grouping column became index, not a separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3db1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this function i can make them separate columns\n",
    "a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an example of counting revenue for every day by every employee\n",
    "# agg functin cant accept math operations inside, so one way around is to make a new column\n",
    "rev_1 = df.groupby(['date', 'employee'])[['retail_price', 'purch_price']].agg('sum')\n",
    "rev_1['revenue'] = rev_1['retail_price'] - rev_1['purch_price']\n",
    "rev_1['revenue'] = rev_1['revenue'].round(2) # here i rounded values in revenue column\n",
    "rev_1 = rev_1.reset_index() # here i reassigned rev datarfame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i sorted it by revenue in descending order (ascending by default)\n",
    "rev_1.sort_values(by='revenue', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way is using 'apply' function after grouping\n",
    "# it will apply everything i put in it to all columns - in this case its 'retail_price' and 'purch_price'\n",
    "rev_2 = df.groupby(['date', 'employee'])[['retail_price', 'purch_price', 'amnt']].apply(lambda x: sum(x['amnt'] * (x['retail_price'] - x['purch_price'])))\n",
    "# in this case x in lambda is a row in a column\n",
    "rev_2 = rev_2.reset_index()\n",
    "# this way is better becouse it is easier to add new variables such as amnt in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed names for columns\n",
    "rev_2.columns = ['date', 'employee', 'revenue']\n",
    "rev_2['revenue'] = rev_2['revenue'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted by date and revenue in descending order\n",
    "rev_2.sort_values(['date','revenue'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de91783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how i can round by hundreds\n",
    "df['r_retail_price'] = df.retail_price.apply(lambda x: round(x, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = df.groupby(['date', 'r_retail_price'])['amnt'].agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make scatterplot i can use seaborn\n",
    "sns.scatterplot(data=diag, x='r_retail_price', y='amnt', hue='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=diag, x='r_retail_price', y='amnt', hue='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this data to look more clean and readable i can make pivot tables \n",
    "df.groupby(['date', 'apt', 'employee', 'pay_type'])[['amnt']].agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86eecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = df.groupby(['date', 'apt', 'employee', 'pay_type'])[['amnt']].agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.pivot_table(values='amnt', index=['date', 'apt', 'employee'], columns='pay_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i can also do pivot table from initial dataframe, without groupby and agg steps\n",
    "df.pivot_table(values='amnt', index=['date', 'apt', 'employee'], columns='pay_type', aggfunc='sum')\n",
    "# i just need to add one extra argument 'aggfunc' - it will tel what type of agg i need for values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i also can unpivot tables\n",
    "pvt = df.pivot_table(values='amnt', index=['date', 'apt', 'employee'], columns='pay_type', aggfunc='sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b67356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_vars is the same as index above, what stays unchanged\n",
    "# value_vars is what i need to unpivot - change from columns to one column 'pay_type'\n",
    "pd.melt(pvt, id_vars=['date', 'apt', 'employee'], value_vars=[15, 18], var_name='changed_back_pay_type')\n",
    "# last argument is for changing name for a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both these functions - pivot_table and melt are often used in tasks like changing long table to wide and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bc093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if i need to find something specific like 'positions which sold more then 5 times' or 'where price is higher then ...' i can use this\n",
    "# like in sql 'where something something'\n",
    "df.iloc[5,5] # this command will return 5th row in 5th column\n",
    "df.iloc[:5, 5] # this will return all rows till 5th (not including it) in 5th column\n",
    "df.iloc[:5, :5] # all rows and columns before 5\n",
    "df.iloc[1:5, 2:4] # this will give from 1 to 5 rows in 2 to 4 columns (not including last one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc only works with indexes, if i want to search by column names i can use loc\n",
    "df.loc[2:5, ['date', 'apt']] # i need rows from 2 to 5 in columns 'date' and 'apt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so to look by some specific parameter line 'where something something' i can use this\n",
    "df['amnt'] > 5 # this will return massive with True\\False, so i need to index that\n",
    "df[df['amnt']>5] # this will return all rows where value in column 'amnt' is more then 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i can use 'and - &' or 'or - |' \n",
    "# every conditions must be in ()\n",
    "df[(df['amnt'] > 5) | (df['retail_price'] > 2000)]\n",
    "df[(df['amnt'] > 5) & (df['retail_price'] > 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if i want to see only rows where 'pay_type' is not 15 and not 20\n",
    "df[df.loc[: , 'pay_type'].isin([15, 20])] # this function check if values in column are in massive and returns bool df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.loc[: , 'pay_type'].isin([15, 20])] #by adding ~ in front of the condition i can reverse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f467dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this filtering is a good practice with building scatterplot\n",
    "diag = df.groupby(['date', 'r_retail_price'])['amnt'].agg('sum').reset_index()\n",
    "diag = diag[diag.r_retail_price < 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=diag, x='r_retail_price', y='amnt', hue='date' )\n",
    "# i discarded spikes from scatterplot and looks much better now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
